"""Export CAR-T Intelligence Agent query results to Markdown and JSON.

Provides two public functions:
  - export_markdown() — human-readable report with evidence tables and citations
  - export_json()     — machine-readable structured data using Pydantic serialization

No external dependencies beyond what the project already uses (Pydantic, stdlib).

Author: Adam Jones
Date: February 2026
"""

import json
from datetime import datetime, timezone
from typing import Optional

from .models import ComparativeResult, CrossCollectionResult, SearchHit


VERSION = "1.2.0"


# ═══════════════════════════════════════════════════════════════════════
# PUBLIC API
# ═══════════════════════════════════════════════════════════════════════


def generate_filename(extension: str) -> str:
    """Generate a timestamped filename for export.

    Args:
        extension: File extension without dot (e.g. "md", "json")

    Returns:
        Filename like cart_query_20260219_143025.md
    """
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"cart_query_{ts}.{extension}"


def export_markdown(
    query: str,
    response_text: str,
    evidence: Optional[CrossCollectionResult] = None,
    comp_result: Optional[ComparativeResult] = None,
    filters_applied: Optional[dict] = None,
) -> str:
    """Export a query result as a Markdown report.

    Args:
        query: The user's original question
        response_text: The LLM-generated response
        evidence: CrossCollectionResult (for standard queries)
        comp_result: ComparativeResult (for comparative queries)
        filters_applied: Dict of sidebar filters that were active

    Returns:
        Complete Markdown report as a string
    """
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    filters_str = _format_filters(filters_applied)

    lines = [
        "# CAR-T Intelligence Report",
        "",
        f"**Query:** {query}",
        f"**Generated:** {timestamp}",
        f"**Filters:** {filters_str}",
        "",
        "---",
        "",
        "## Response",
        "",
        response_text,
        "",
        "---",
        "",
    ]

    # Evidence section
    if comp_result and comp_result.total_hits > 0:
        lines.append("## Evidence Sources (Comparative)")
        lines.append("")
        lines.append(f"### {comp_result.entity_a}")
        lines.append("")
        lines.extend(_format_evidence_section(comp_result.evidence_a))
        lines.append("")
        lines.append(f"### {comp_result.entity_b}")
        lines.append("")
        lines.extend(_format_evidence_section(comp_result.evidence_b))

        if comp_result.comparison_context:
            lines.append("")
            lines.append("## Knowledge Graph Context")
            lines.append("")
            lines.append(comp_result.comparison_context)

        # Search metrics
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append("## Search Metrics")
        lines.append("")
        lines.append("| Metric | Value |")
        lines.append("|--------|-------|")
        lines.append(f"| Total Results | {comp_result.total_hits} |")
        lines.append(f"| {comp_result.entity_a} Results | {comp_result.evidence_a.hit_count} |")
        lines.append(f"| {comp_result.entity_b} Results | {comp_result.evidence_b.hit_count} |")
        lines.append(f"| Search Time | {comp_result.total_search_time_ms:.0f}ms |")

    elif evidence and evidence.hit_count > 0:
        lines.append("## Evidence Sources")
        lines.append("")
        lines.extend(_format_evidence_section(evidence))

        if evidence.knowledge_context:
            lines.append("")
            lines.append("## Knowledge Graph Context")
            lines.append("")
            lines.append(evidence.knowledge_context)

        # Search metrics
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append("## Search Metrics")
        lines.append("")
        lines.append("| Metric | Value |")
        lines.append("|--------|-------|")
        lines.append(f"| Total Results | {evidence.hit_count} |")
        lines.append(f"| Collections Searched | {evidence.total_collections_searched} |")
        lines.append(f"| Search Time | {evidence.search_time_ms:.0f}ms |")

    # Footer
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append(f"*Generated by HCLS AI Factory — CAR-T Intelligence Agent v{VERSION}*")
    lines.append("")

    return "\n".join(lines)


def export_json(
    query: str,
    response_text: str,
    evidence: Optional[CrossCollectionResult] = None,
    comp_result: Optional[ComparativeResult] = None,
    filters_applied: Optional[dict] = None,
) -> str:
    """Export a query result as structured JSON.

    Uses Pydantic .model_dump() for proper serialization of evidence models.

    Args:
        query: The user's original question
        response_text: The LLM-generated response
        evidence: CrossCollectionResult (for standard queries)
        comp_result: ComparativeResult (for comparative queries)
        filters_applied: Dict of sidebar filters that were active

    Returns:
        Pretty-printed JSON string
    """
    is_comparative = comp_result is not None and comp_result.total_hits > 0
    timestamp = datetime.now(timezone.utc).isoformat()

    data = {
        "report_type": "cart_intelligence_query",
        "version": VERSION,
        "generated_at": timestamp,
        "query": query,
        "response": response_text,
        "is_comparative": is_comparative,
        "filters_applied": filters_applied or {},
    }

    if is_comparative:
        data["comparative"] = {
            "entity_a": comp_result.entity_a,
            "entity_b": comp_result.entity_b,
            "evidence_a": comp_result.evidence_a.model_dump(),
            "evidence_b": comp_result.evidence_b.model_dump(),
            "comparison_context": comp_result.comparison_context,
        }
        data["search_metrics"] = {
            "total_results": comp_result.total_hits,
            "entity_a_results": comp_result.evidence_a.hit_count,
            "entity_b_results": comp_result.evidence_b.hit_count,
            "search_time_ms": round(comp_result.total_search_time_ms, 1),
        }
    elif evidence:
        data["evidence"] = evidence.model_dump()
        data["search_metrics"] = {
            "total_results": evidence.hit_count,
            "collections_searched": evidence.total_collections_searched,
            "search_time_ms": round(evidence.search_time_ms, 1),
        }

    return json.dumps(data, indent=2, default=str)


# ═══════════════════════════════════════════════════════════════════════
# PRIVATE HELPERS
# ═══════════════════════════════════════════════════════════════════════


def _format_filters(filters_applied: Optional[dict]) -> str:
    """Format the sidebar filters for display."""
    if not filters_applied:
        return "None"
    parts = []
    for key, value in filters_applied.items():
        if value and value not in ("All Targets", "All Stages"):
            parts.append(f"{key}: {value}")
    return ", ".join(parts) if parts else "None"


def _format_citation_link(collection: str, record_id: str) -> str:
    """Format a clickable citation link (mirrors rag_engine._format_citation)."""
    if collection == "Literature" and record_id.isdigit():
        return f"[PMID {record_id}](https://pubmed.ncbi.nlm.nih.gov/{record_id}/)"
    if collection == "Trial" and record_id.upper().startswith("NCT"):
        return f"[{record_id}](https://clinicaltrials.gov/study/{record_id})"
    return record_id


def _format_evidence_section(evidence: CrossCollectionResult) -> list[str]:
    """Format all evidence for a CrossCollectionResult, grouped by collection."""
    lines = []
    by_coll = evidence.hits_by_collection()
    for coll_name, hits in by_coll.items():
        lines.extend(_format_evidence_table(hits, coll_name))
        lines.append("")
    return lines


def _format_evidence_table(hits: list[SearchHit], collection_name: str) -> list[str]:
    """Format a Markdown table for hits from a single collection.

    Uses collection-specific columns to surface the most relevant metadata.
    """
    lines = [f"### {collection_name} ({len(hits)} results)", ""]

    if collection_name == "Literature":
        lines.append("| # | ID | Score | Source | Title | Year | Target | Journal |")
        lines.append("|---|-----|-------|--------|-------|------|--------|---------|")
        for i, hit in enumerate(hits[:10], 1):
            m = hit.metadata
            link = _format_citation_link(hit.collection, hit.id)
            title = m.get("title", "")[:60]
            year = m.get("year", "")
            target = m.get("target_antigen", "")
            journal = m.get("journal", "")[:30]
            lines.append(f"| {i} | {hit.id} | {hit.score:.3f} | {link} | {title} | {year} | {target} | {journal} |")

    elif collection_name == "Trial":
        lines.append("| # | NCT ID | Score | Source | Phase | Status | Sponsor | Enrollment |")
        lines.append("|---|--------|-------|--------|-------|--------|---------|------------|")
        for i, hit in enumerate(hits[:10], 1):
            m = hit.metadata
            link = _format_citation_link(hit.collection, hit.id)
            phase = m.get("phase", "")
            status = m.get("status", "")
            sponsor = m.get("sponsor", "")[:30]
            enrollment = m.get("enrollment", "")
            lines.append(f"| {i} | {hit.id} | {hit.score:.3f} | {link} | {phase} | {status} | {sponsor} | {enrollment} |")

    elif collection_name == "Construct":
        lines.append("| # | ID | Score | Name | Generation | Costimulatory | FDA Status |")
        lines.append("|---|-----|-------|------|------------|---------------|------------|")
        for i, hit in enumerate(hits[:10], 1):
            m = hit.metadata
            name = m.get("name", "")[:30]
            gen = m.get("generation", "")
            costim = m.get("costimulatory_domain", "")
            fda = m.get("fda_status", "")
            lines.append(f"| {i} | {hit.id} | {hit.score:.3f} | {name} | {gen} | {costim} | {fda} |")

    elif collection_name == "Assay":
        lines.append("| # | ID | Score | Type | Cell Line | Metric | Value | Outcome |")
        lines.append("|---|-----|-------|------|-----------|--------|-------|---------|")
        for i, hit in enumerate(hits[:10], 1):
            m = hit.metadata
            atype = m.get("assay_type", "")
            cell = m.get("cell_line", "")
            metric = m.get("key_metric", "")
            value = m.get("metric_value", "")
            outcome = m.get("outcome", "")
            lines.append(f"| {i} | {hit.id} | {hit.score:.3f} | {atype} | {cell} | {metric} | {value} | {outcome} |")

    elif collection_name == "Manufacturing":
        lines.append("| # | ID | Score | Process Step | Parameter | Batch |")
        lines.append("|---|-----|-------|-------------|-----------|-------|")
        for i, hit in enumerate(hits[:10], 1):
            m = hit.metadata
            step = m.get("process_step", "")
            param = m.get("parameter", "")
            batch = m.get("batch_id", "")
            lines.append(f"| {i} | {hit.id} | {hit.score:.3f} | {step} | {param} | {batch} |")

    else:
        # Generic fallback
        lines.append("| # | ID | Score | Text |")
        lines.append("|---|-----|-------|------|")
        for i, hit in enumerate(hits[:10], 1):
            text = hit.text[:80].replace("|", "\\|")
            lines.append(f"| {i} | {hit.id} | {hit.score:.3f} | {text} |")

    return lines
